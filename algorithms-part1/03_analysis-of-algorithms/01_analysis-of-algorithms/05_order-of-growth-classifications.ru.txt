К счастью, при анализе
алгоритмов используется не так много различных функций.
Благодаря этому можно классифицировать алгоритмы по их производительности
в зависимости от масштабов задачи. Об этом и поговорим. Хорошая новость
в том, что нам нужны всего несколько функций.
Конечно, используются и другие, но подавляющее большинство алгоритмов,
которые мы рассматриваем, описываются этими несколькими
функциями, представленными на графике.
Когда мы говорим о скорости (порядке) роста, то не обсуждаем ведущую константу.
 Обычно говорим, что время работы алгоритма пропорционально N*Log(N).
 То есть, мы предполагаем,
что время выполнения равно ~C*N*Log(N), где C - некоторая константа.
Эти графики, которые построены в логарифмических координатах, дают хорошее представление о том, 
что происходит. 
Если порядок роста логарифмический или константа, не имеет значения, насколько велика задача.
 Время работы будет небольшим. Если для N=1000 время равно T,
то для N=500.000 время по-прежнему близко к T. При линейной зависимости,
если порядок роста пропорционален N, тогда по мере роста N время работы
 увеличивается соответственно. Так же для N*log(N).
К подобным алгоритмам мы стремимся. Время работы пропорционально
размерам входных данных. Это приемлемая ситуация.
Ранее мы говорили о Union-Find: если график оказывается
квадратичным, время работы растет гораздо быстрее, чем размер входных данных.
Для больших объемов данных это не подходит. Если зависимость кубическая,
то ещё хуже. Нашей главной задачей является
определение степени функции. Такая классификация
по степени проистекает из кода. Если в коде нет циклов,
тогда рост будет линейным. Если в коде есть некий цикл,
в котором входные данные делятся пополам, например,
алгоритм бинарного поиска, то порядок роста будет логарифмическим.
Мы его ещё проанализируем. Если объем данных удваивается,
то время растет линейно. Если вы удваиваете огромный объем данных,
то время..простите, не растет линейно, а почти не меняется.
Изменение в lg(N) раз едва заметно. Если вы перебираете все элементы массива,
то время меняется пропорционально N. Типичным примером будет поиск максимума, подсчет числа нулей или задача 1Sum. Очень интересной категорией является так называемые
N*Log(N) алгоритмы или квазилинейные алгоритмы. Для их получения используют
специальный принцип "разделяй и властвуй".
 И алгоритм сортировки слиянием (Mergesort), о котором мы поговорим через пару недель, является главным примером
таких алгоритмов. Если возникает двойной цикл for, как в алгоритме 2-SUM,
время работы пропорционально N^2. Как мы видели, оно квадратично.
Если тройной цикл for, как в алгоритме 3-SUM, то алгоритм становится кубическим
- время работы пропорционально N^3. Для квадратичных или кубических алгоритмов множитель
соответственно 4 или 8, т.е. при удвоении N время работы для кубического алгоритма
увеличивается в 8 раз. У вас будет время посчитать это,
пока ждете завершения работы программы. Также есть категория алгоритмов,
время работы которых экспоненциально, N растет в узких пределах.
Мы поговорим о них в конце второй части курса. Мы рассмотрели порядок роста и потратили слишком много времени.
Сосредоточимся на интересующих нас алгоритмах, а именно тех,
что решают задачи больших масштабов — линейных и N*Log(N).
 Ведь даже на быстрых компьютерах квадратичные алгоритмы могут
решать задачи для N=10^4, а кубические алгоритмы — для N=10^3. Они просто не годятся для сегодняшних
объемов информации Этот факт становится всё более очевидным с течением времени.
Можно дискутировать по поводу полезности таких алгоритмов,
но ситуация лишь усугубляется. Нужны более быстрые алгоритмы.
Проиллюстрируем разработку математической модели,
которая описывает производительность алгоритма, на примере алгоритма двоичного поиска.
Задача у него такая: есть сортированный массив целых чисел,
нужно определить, содержится ли в нем определенный элемент.
И если да, то под каким индексом. Двоичный алгоритм быстро решает эту задачу.
Он сравнивает элемент со средним значением. В данном случае, мы ищем 33
и сравниваем его с 53. Если значение меньше, то продолжаем искать
в левой половине. Если больше, то поиск надо продолжать в правой половине массива.
При равенстве ответ найден. Алгоритм применяется рекурсивно.
Посмотрим демонстрацию. Ищем 33 в массиве.
Сравниваем со значением в средней точке массива. Там 53, поэтому остается левая половина массива. Находим значение в середине левой части Это 25. 33 больше,
 поэтому сдвигаемся вправо. Остается подмассив между 25 и 53. Смотрим в середину - 33 меньше,
 поэтому мы двигаемся влево. Остался последний элемент, и это 33. Его индекс в массиве - 4.
 Если ищем элемент, которого нет, то процесс повторяется. Скажем, мы ищем 34. Действия те же. Проверяем левую половину, затем направо.
Левее от 43. Остался последний элемент, и это не 34.
Значит элемент не в массиве. Вот код двоичного поиска.
Хотя двоичный поиск — простой алгоритм,
весьма непросто написать код правильно. Насколько я знаю, правильный код
был написан только в 1962 г. А в 2006 году была обнаружена ошибка
реализации двоичного поиска в Java. Лишнее напоминание о том,
насколько внимательно надо подходить к алгоритмам, особенно для библиотек,
 которые будут использоваться миллионами людей. Вот реализация. Она не рекурсивная,
хотя можно реализовать и рекурсивную. Она просто отражает в коде то,
что я описывал словами. Ищем, находится ли ключ в массиве.
Мы используем два указателя - нижний и верхний, для указания интересующей нас
части массива. Пока нижний указатель меньше или равен верхнему,
мы вычисляем середину. Сравниваем ключ
со значением в середине. Если они равны, выводим индекс средней точки.
 Если меньше - переустанавливаем верхний указатель. Если больше - переустанавливаем нижний указатель.
До тех пор, пока они не сравняются. И если они равны, а мы не нашли ключ,
то выводим "-1". Легко убедиться, что эта программа работает как заявлено,
 размышляя над этим инвариантом. Если ключ в массиве, то он между
нижним и верхним указателем. Вы вероятно знакомы с такой программой.
Посмотрим на её математический анализ. Теорема доказывается легко,
и стоит это сделать. Теорема: Двоичный поиск использует
1+log2(N) сравнений для завершения поиска в отсортированном
массиве размера N. Для решения введем переменную T(N), которая обозначает число сравнений,
 необходимых алгоритму для поиска в массиве размера N И затем мы записываем рекуррентное отношение,
которое отражает код. А код работает так:
он делит задачу пополам так, что если T(n) меньше или равно
T(N/2)+..зависит от того, как вы считаете. Дихотомическое сравнение,
 следовательно массив делится пополам при выполнении одного сравнения.
 И это истинно, если размер массива больше единицы. Если размер равен единице, то решение — 1.
 Итак, это рекуррентное отношение, описывающее вычисление. Решим это выражение методом рекурсии по отношению к условию справа.⏎Это называют экономизацией. Итак, если это истинно,
 мы можем применить то же соотношение к T(N/2) и произвести еще одну единицу. И если это истинно,
повторить для T(N/4) и произвести ещё одну единицу. И так далее, пока мы получим 1 (в N).
 К этому моменту слева будет выписано Lg(N) единиц. Вы, возможно, заметили, что доказательство справедливо, если N является степенью двойки.
Мы не указали, что будет при нечетном N.
Можно и в этом случае доказать, что время работы алгоритма
двоичного поиска всегда логарифмическое.
На основе этого можно разработать более быстрый алгоритм для 3-SUM.
 Это алгоритм, основанный на сортировке. Берем числа из входного набора
и сортируем их. Подробнее поговорим на следующей неделе.
Время получается пропорциональным N*log(N).
Основная часть вычислений приходится на двоичный поиск для -ai и aj для каждой пары чисел (a[i], a[j]). Если такое число находится, то у нас есть три числа,
чья сумма равна нулю. Сортируем числа, проходим по каждой паре, выполняем двоичный поиск
и находим решение для задачи 3-SUM.
 И так делаем для всех пар чисел. Быстрый анализ говорит, что порядок роста времени выполнения
 должен оказываться N^2 * Log(N). Достаточно использовать
простейшую сортировку. Но время работы двоичного поиска
для каждой пары, каждой из N^2 пар, или (N^2/2) пар, для которых мы собираемся
 провести двоичный поиск [Lon(n)], таким образом, получаем время работы N^2*Log(N).
 Это быстрый пример того, как можно улучшить производительность, найти улучшенный алгоритм решения задачи.
N^2Log(N) гораздо меньше, чем N^3 для больших N.
 Таким образом, делаем предположение, что при использовании сортировки
и двоичного поиска, получим более быструю программу.
Можно провести опыт и обнаружить, что решение задачи
при объеме данных 8000 чисел занимало 50 с, а стало занимать меньше секунды.
За 50 секунд мы можем решить задачу размером до 64.000 чисел.
Более низкий порядок роста означает более высокую скорость работы.
При анализе алгоритмов мы можем проводить тесты и вычислять,
какой алгоритм лучше. Конечно, переходя от N^3 к N^2Log(N)
мы рассчитываем получить более быстрый алгоритм.