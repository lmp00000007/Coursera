1
00:00:03,064 --> 00:00:08,071
К счастью, при анализе
алгоритмов используется

2
00:00:08,071 --> 00:00:14,746
не так много различных функций.
Благодаря этому можно классифицировать

3
00:00:14,746 --> 00:00:20,978
алгоритмы по их производительности
в зависимости от масштабов задачи.

4
00:00:20,978 --> 00:00:27,173
Об этом и поговорим. Хорошая новость
в том, что нам нужны

5
00:00:27,173 --> 00:00:31,915
всего несколько функций.
Конечно, используются и другие,

6
00:00:31,915 --> 00:00:37,479
но подавляющее большинство алгоритмов,
которые мы рассматриваем,

7
00:00:37,479 --> 00:00:43,394
описываются этими несколькими
функциями, представленными

8
00:00:43,394 --> 00:00:50,362
на графике.
Когда мы говорим о скорости (порядке) роста,

9
00:00:50,362 --> 00:00:55,793
то не обсуждаем ведущую константу.
 Обычно говорим, что время работы

10
00:00:55,793 --> 00:01:00,971
алгоритма пропорционально N*Log(N).
 То есть,

11
00:01:00,971 --> 00:01:06,686
мы предполагаем,
что время выполнения равно ~C*N*Log(N),

12
00:01:06,686 --> 00:01:12,671
где C - некоторая константа.
Эти графики, которые построены в логарифмических координатах,

13
00:01:12,671 --> 00:01:18,801
дают хорошее представление о том, 
что происходит. 
Если порядок роста логарифмический

14
00:01:18,801 --> 00:01:25,026
или константа, не имеет значения, насколько велика задача.
 Время работы будет небольшим.

15
00:01:25,026 --> 00:01:32,082
Если для N=1000 время равно T,
то для N=500.000 время по-прежнему близко к T.

16
00:01:32,082 --> 00:01:38,674
При линейной зависимости,
если порядок роста пропорционален N, тогда

17
00:01:38,674 --> 00:01:44,945
по мере роста N время работы
 увеличивается соответственно.

18
00:01:44,945 --> 00:01:51,031
Так же для N*log(N).
К подобным алгоритмам мы стремимся.

19
00:01:51,031 --> 00:01:56,755
Время работы пропорционально
размерам входных данных.

20
00:01:56,755 --> 00:02:02,647
Это приемлемая ситуация.
Ранее мы говорили

21
00:02:02,647 --> 00:02:07,843
о Union-Find: если график оказывается
квадратичным, время работы

22
00:02:07,843 --> 00:02:13,469
растет гораздо быстрее, чем размер входных данных.
Для больших объемов данных

23
00:02:13,469 --> 00:02:21,397
это не подходит. Если зависимость кубическая,
то ещё хуже.

24
00:02:21,397 --> 00:02:28,515
Нашей главной задачей является
определение степени функции.

25
00:02:28,515 --> 00:02:35,708
Такая классификация
по степени проистекает из кода.

26
00:02:35,708 --> 00:02:41,918
Если в коде нет циклов,
тогда рост будет линейным.

27
00:02:41,918 --> 00:02:49,033
Если в коде есть некий цикл,
в котором входные данные

28
00:02:49,033 --> 00:02:54,276
делятся пополам, например,
алгоритм бинарного поиска,

29
00:02:54,276 --> 00:03:00,676
то порядок роста будет логарифмическим.
Мы его ещё проанализируем.

30
00:03:00,676 --> 00:03:06,879
Если объем данных удваивается,
то время растет линейно.

31
00:03:06,879 --> 00:03:12,633
Если вы удваиваете огромный объем данных,
то время..простите, не растет линейно,

32
00:03:12,633 --> 00:03:18,252
а почти не меняется.
Изменение в lg(N) раз едва заметно.

33
00:03:18,252 --> 00:03:25,070
Если вы перебираете все элементы массива,
то время меняется

34
00:03:25,070 --> 00:03:31,592
пропорционально N. Типичным примером будет поиск максимума,

35
00:03:31,592 --> 00:03:38,205
подсчет числа нулей или задача 1Sum.

36
00:03:38,205 --> 00:03:44,681
Очень интересной категорией является так называемые
N*Log(N) алгоритмы или квазилинейные алгоритмы.

37
00:03:44,681 --> 00:03:50,143
Для их получения используют
специальный принцип

38
00:03:50,143 --> 00:03:55,573
"разделяй и властвуй".
 И алгоритм сортировки слиянием (Mergesort), о котором мы поговорим

39
00:03:55,573 --> 00:04:01,012
через пару недель, является главным примером
таких алгоритмов.

40
00:04:01,012 --> 00:04:07,543
Если возникает двойной цикл for, как в алгоритме 2-SUM,
время работы пропорционально N^2.

41
00:04:07,543 --> 00:04:13,530
Как мы видели, оно квадратично.
Если тройной цикл for, как в алгоритме 3-SUM,

42
00:04:13,530 --> 00:04:19,369
то алгоритм становится кубическим
- время работы пропорционально N^3. Для квадратичных

43
00:04:19,369 --> 00:04:25,312
или кубических алгоритмов множитель
соответственно 4 или 8, т.е. при удвоении N

44
00:04:25,312 --> 00:04:30,331
время работы для кубического алгоритма
увеличивается в 8 раз.

45
00:04:30,331 --> 00:04:35,952
У вас будет время посчитать это,
пока ждете завершения работы программы.

46
00:04:35,952 --> 00:04:41,645
Также есть категория алгоритмов,
время работы которых экспоненциально,

47
00:04:41,645 --> 00:04:47,059
N растет в узких пределах.
Мы поговорим о них в конце второй части курса.

48
00:04:47,059 --> 00:04:53,150
Мы рассмотрели порядок роста

49
00:04:53,150 --> 00:04:59,352
и потратили слишком много времени.
Сосредоточимся на интересующих

50
00:04:59,352 --> 00:05:04,723
нас алгоритмах, а именно тех,
что решают задачи больших масштабов —

51
00:05:04,723 --> 00:05:11,633
линейных и N*Log(N).
 Ведь даже на быстрых компьютерах

52
00:05:11,635 --> 00:05:17,913
квадратичные алгоритмы могут
решать задачи для N=10^4,

53
00:05:17,913 --> 00:05:23,246
а кубические алгоритмы — для N=10^3.

54
00:05:23,246 --> 00:05:28,567
Они просто не годятся для сегодняшних
объемов информации

55
00:05:28,567 --> 00:05:34,689
Этот факт становится всё более очевидным

56
00:05:34,689 --> 00:05:41,269
с течением времени.
Можно дискутировать по поводу

57
00:05:41,269 --> 00:05:47,154
полезности таких алгоритмов,
но ситуация лишь усугубляется.

58
00:05:47,154 --> 00:05:52,593
Нужны более быстрые алгоритмы.
Проиллюстрируем разработку

59
00:05:52,593 --> 00:05:57,756
математической модели,
которая описывает производительность алгоритма,

60
00:05:57,756 --> 00:06:03,037
на примере алгоритма двоичного поиска.
Задача у него такая:

61
00:06:03,037 --> 00:06:08,323
есть сортированный массив целых чисел,
нужно определить, содержится ли в нем

62
00:06:08,323 --> 00:06:13,321
определенный элемент.
И если да, то под каким индексом.

63
00:06:13,321 --> 00:06:18,178
Двоичный алгоритм быстро решает эту задачу.
Он сравнивает элемент со средним значением.

64
00:06:18,178 --> 00:06:22,941
В данном случае, мы ищем 33
и сравниваем его с 53.

65
00:06:22,941 --> 00:06:27,737
Если значение меньше, то продолжаем искать
в левой половине. Если больше,

66
00:06:27,737 --> 00:06:32,819
то поиск надо продолжать в правой половине массива.
При равенстве ответ найден.

67
00:06:32,819 --> 00:06:39,680
Алгоритм применяется рекурсивно.
Посмотрим демонстрацию.

68
00:06:39,680 --> 00:06:45,777
Ищем 33 в массиве.
Сравниваем со значением в средней точке массива. Там 53,

69
00:06:45,777 --> 00:06:51,160
поэтому остается левая половина массива.

70
00:06:51,160 --> 00:06:56,788
Находим значение в середине левой части Это 25. 33 больше,
 поэтому сдвигаемся вправо.

71
00:06:56,788 --> 00:07:02,399
Остается подмассив между 25 и 53.

72
00:07:02,399 --> 00:07:08,690
Смотрим в середину - 33 меньше,
 поэтому мы двигаемся влево.

73
00:07:08,690 --> 00:07:15,019
Остался последний элемент, и это 33.

74
00:07:15,019 --> 00:07:21,234
Его индекс в массиве - 4.
 Если ищем элемент, которого нет, то процесс повторяется.

75
00:07:21,234 --> 00:07:26,874
Скажем, мы ищем 34. Действия те же.

76
00:07:26,874 --> 00:07:32,923
Проверяем левую половину, затем направо.
Левее от 43.

77
00:07:32,923 --> 00:07:39,478
Остался последний элемент, и это не 34.
Значит элемент не в массиве.

78
00:07:39,478 --> 00:07:47,488
Вот код двоичного поиска.
Хотя двоичный поиск —

79
00:07:47,488 --> 00:07:53,391
простой алгоритм,
весьма непросто написать код правильно.

80
00:07:53,391 --> 00:07:58,827
Насколько я знаю, правильный код
был написан только в 1962 г.

81
00:07:58,827 --> 00:08:04,430
А в 2006 году была обнаружена ошибка
реализации двоичного поиска в Java.

82
00:08:04,430 --> 00:08:09,417
Лишнее напоминание о том,
насколько внимательно надо подходить к алгоритмам,

83
00:08:09,417 --> 00:08:15,847
особенно для библиотек,
 которые будут использоваться миллионами людей.

84
00:08:15,847 --> 00:08:24,048
Вот реализация. Она не рекурсивная,
хотя можно реализовать и рекурсивную.

85
00:08:24,048 --> 00:08:32,050
Она просто отражает в коде то,
что я описывал словами.

86
00:08:32,050 --> 00:08:41,029
Ищем, находится ли ключ в массиве.
Мы используем два указателя - нижний и верхний,

87
00:08:41,029 --> 00:08:46,051
для указания интересующей нас
части массива. Пока

88
00:08:46,051 --> 00:08:51,053
нижний указатель меньше или равен верхнему,
мы вычисляем середину.

89
00:08:51,053 --> 00:08:56,082
Сравниваем ключ
со значением в середине. Если они равны,

90
00:08:56,082 --> 00:09:02,011
выводим индекс средней точки.
 Если меньше - переустанавливаем верхний указатель.

91
00:09:02,011 --> 00:09:07,026
Если больше - переустанавливаем нижний указатель.
До тех пор, пока они не сравняются.

92
00:09:07,026 --> 00:09:12,034
И если они равны, а мы не нашли ключ,
то выводим "-1".

93
00:09:12,034 --> 00:09:18,023
Легко убедиться, что эта программа работает как заявлено,
 размышляя над этим инвариантом.

94
00:09:18,023 --> 00:09:24,025
Если ключ в массиве, то он между
нижним и верхним указателем.

95
00:09:24,025 --> 00:09:30,036
Вы вероятно знакомы с такой программой.
Посмотрим

96
00:09:30,036 --> 00:09:36,039
на её математический анализ.

97
00:09:36,039 --> 00:09:42,049
Теорема доказывается легко,
и стоит это сделать.

98
00:09:42,049 --> 00:09:48,045
Теорема: Двоичный поиск использует
1+log2(N) сравнений

99
00:09:48,045 --> 00:09:55,032
для завершения поиска в отсортированном
массиве размера N. Для решения

100
00:09:55,032 --> 00:10:02,089
введем переменную T(N), которая обозначает число сравнений,
 необходимых алгоритму для поиска в массиве размера N

101
00:10:02,089 --> 00:10:10,001
И затем мы записываем рекуррентное отношение,
которое отражает код.

102
00:10:10,001 --> 00:10:16,076
А код работает так:
он делит задачу пополам так,

103
00:10:16,076 --> 00:10:23,000
что если T(n) меньше или равно
T(N/2)+..зависит от того, как вы считаете.

104
00:10:23,000 --> 00:10:29,045
Дихотомическое сравнение,
 следовательно массив делится пополам

105
00:10:29,045 --> 00:10:35,046
при выполнении одного сравнения.
 И это истинно, если размер массива больше единицы.

106
00:10:35,046 --> 00:10:42,039
Если размер равен единице, то решение — 1.
 Итак, это рекуррентное отношение, описывающее вычисление.

107
00:10:42,039 --> 00:10:48,094
Решим это выражение методом рекурсии 

108
00:10:48,094 --> 00:10:55,882
по отношению к условию справа.⏎Это называют экономизацией.

109
00:10:55,882 --> 00:11:03,378
Итак, если это истинно,
 мы можем применить то же соотношение к T(N/2) и произвести еще одну единицу.

110
00:11:03,378 --> 00:11:09,096
И если это истинно,
повторить для T(N/4) и произвести ещё одну единицу. И так далее,

111
00:11:09,096 --> 00:11:15,340
пока мы получим 1 (в N).
 К этому моменту слева будет выписано Lg(N) единиц.

112
00:11:15,340 --> 00:11:21,794
Вы, возможно, заметили, что доказательство

113
00:11:21,794 --> 00:11:29,118
справедливо, если N является степенью двойки.
Мы не указали, что будет

114
00:11:29,118 --> 00:11:36,985
при нечетном N.
Можно и в этом случае доказать,

115
00:11:36,985 --> 00:11:45,649
что время работы алгоритма
двоичного поиска

116
00:11:45,649 --> 00:11:52,779
всегда логарифмическое.
На основе этого можно разработать

117
00:11:52,779 --> 00:11:58,548
более быстрый алгоритм для 3-SUM.
 Это алгоритм, основанный на сортировке.

118
00:11:58,548 --> 00:12:04,638
Берем числа из входного набора
и сортируем их.

119
00:12:04,638 --> 00:12:11,553
Подробнее поговорим на следующей неделе.
Время получается пропорциональным

120
00:12:11,553 --> 00:12:18,416
N*log(N).
Основная часть вычислений приходится

121
00:12:18,416 --> 00:12:25,682
на двоичный поиск для -ai и aj для каждой пары чисел

122
00:12:25,682 --> 00:12:32,452
(a[i], a[j]). Если такое число находится,

123
00:12:32,452 --> 00:12:41,319
то у нас есть три числа,
чья сумма равна нулю.

124
00:12:41,319 --> 00:12:48,610
Сортируем числа, проходим по каждой паре,

125
00:12:48,611 --> 00:12:55,631
выполняем двоичный поиск
и находим решение

126
00:12:55,631 --> 00:13:02,081
для задачи 3-SUM.
 И так делаем для всех пар чисел.

127
00:13:02,081 --> 00:13:08,243
Быстрый анализ говорит, что порядок роста времени выполнения
 должен оказываться N^2 * Log(N).

128
00:13:08,243 --> 00:13:14,064
Достаточно использовать
простейшую сортировку.

129
00:13:14,064 --> 00:13:20,678
Но время работы двоичного поиска
для каждой пары, каждой из N^2 пар,

130
00:13:20,678 --> 00:13:26,712
или (N^2/2) пар, для которых мы собираемся
 провести двоичный поиск [Lon(n)], таким образом,

131
00:13:26,712 --> 00:13:32,997
получаем время работы N^2*Log(N).
 Это быстрый пример того, как можно улучшить производительность,

132
00:13:32,997 --> 00:13:39,970
найти улучшенный алгоритм решения задачи.
N^2Log(N) гораздо меньше,

133
00:13:39,970 --> 00:13:46,462
чем N^3 для больших N.
 Таким образом, делаем предположение,

134
00:13:46,462 --> 00:13:51,872
что при использовании сортировки
и двоичного поиска,

135
00:13:51,872 --> 00:13:58,257
получим более быструю программу.
Можно провести опыт

136
00:13:58,257 --> 00:14:03,498
и обнаружить, что решение задачи
при объеме данных 8000 чисел занимало 50 с,

137
00:14:03,498 --> 00:14:08,858
а стало занимать меньше секунды.
За 50 секунд мы можем решить задачу

138
00:14:08,858 --> 00:14:15,118
размером до 64.000 чисел.
Более низкий порядок роста означает

139
00:14:15,118 --> 00:14:21,051
более высокую скорость работы.
При анализе алгоритмов мы можем

140
00:14:21,051 --> 00:14:26,731
проводить тесты и вычислять,
какой алгоритм лучше.

141
00:14:26,731 --> 00:14:31,893
Конечно, переходя от N^3 к N^2Log(N)
мы рассчитываем получить более

142
00:14:31,893 --> 00:14:33,003
быстрый алгоритм.