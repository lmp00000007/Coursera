Bienvenidos nuevamente. Hoy practicaremos un poco de 
matemática y un poco de ciencia. No mucho, pero debemos tener una base científica para 
comprender el rendimiento de nuestros algoritmos para desplegarlos en la practica 
adecuadamente. Así que hoy hablaremos acerca de como observar las características 
de rendimiento de los algoritmos. Vamos a ver como generar modelos matemáticos 
y como clasificar los algoritmos de acuerdo a al orden de crecimiento de su tiempo de ejecución. 
Hablaremos un poco acerca de la teoría de algoritmos y también de como analizar el uso de memoria. 
Así que para poner todo esto en perspectiva, hablaremos acerca de estos temas 
desde el punto de vista de diferentes tipos de personajes. El primero es el programador 
que necesita resolver un problema, hacer que funcione y después desplegarlo. 
El segundo es el cliente, que quiere usar lo que sea que el programador hizo para 
ejecutar su trabajo. El tercero es el teórico, el cual es alguien que en verdad quiere 
entender lo que está sucediendo. Y el último es algo así como un equipo, todo el 
bloqueo y tacleo básico que a veces es necesario para, ya sabes, hacer que todas estas cosas se 
realicen. Así que hay un poco de cada uno de estos en la clase de hoy. Y de hecho, 
mientras eres estudiante, debes pensar que probablemente tu ejecutes uno o todos 
estos papeles algún día. Así que es muy importante entender los diferentes puntos de vista. 
Así que, la clave en la que nos enfocaremos es el tiempo de ejecución. Incluso la
idea de comprender el tiempo de ejecución de una computación data desde Babbage 
y probablemente antes. He aquí una frase de Babbage, "Tan pronto como 
exista una máquina analítica, este necesariamente guiará el futuro curso de
la ciencia. Cuando se busque un resultado por medio de su ayuda, surgirá la pregunta con que curso
de cálculo puede llegar a esos resultados la máquina en el menor tiempo". 
Si observan la máquina de Babbage llamada el motor analítico, cuenta con una 
palanca. Y literalmente la preocupación de Babbage, sabiendo cuanto tomaría una 
computación, era cuantas veces se debía dar vuelta a la palanca.
No es tan diferente en la actualidad. La palanca puede ser algo electrónico sucediendo millones de veces por segundo. Pero aun así, continuamos buscando, cuantas veces 
se necesita ejecutar una operación discreta para lograr terminar una computación.
Existen varias razones para analizar algoritmos. En el contexto de este
curso, estamos principalmente interesados en una predicción con buena performance. Y también queremos
comparar el performance de distintos algoritmos para la misma tarea, y ser
capaces de proveer algunos garantías de que tan bien se desempeñan. Además, está entender algunas bases teóricas para saber como se desempeñan los algoritmos. Pero principalmente, la razón práctica por la cual deseamos analizar y entender los algoritmos es para evitar errores de funcionamiento. Queremos tener confianza en nuestros algoritmos para que culminen las tareas en el período de tiemo, que, que pensamos lo harán. Y es muy, muy frecuente ver, en la infraestructura computacional de hoy, una situación, donde el cliente obtiene un mal rendimiento, porque el programador no entendió las características de funcionamiento del algoritmo. Y la lección de hoy es sobre tratar de evitar eso. Ahora, nos vamos a enfocar en el desempeño y en la comparación de algoritmos en este curso. Hay cursos posteriores en el currículo típico de las Ciencias de la Computación que tienen más información sobre las bases teóricas de algoritmos y yo voy a mencionar un poco sobre eso después. Pero nuestro enfoque está en poder predecir el rendimiento y comparar algoritmos. Ahora, hay una larga lista de casos de éxito en diseñar algoritmos con mejor desempeño en, en permitir la solución de problemas que de otra manera no podrían resolverse. Y yo simplemente daré un par de Ejemplos. Uno de los primeros y más famosos es el llamado algoritmo FFT. Que es un algoritmo para descomponer la forma de onda de n muestras de señal en componentes periódicos. Y esa es la base de DVDs y JPEGs y muchas otras aplicaciones Hay una manera fácil de hacerlo, que toma un tiempo proporcional a N^2. Pero el algoritmo FFT, toma solo N log N pasos. Y la diferencia entre N log N y N^2 es la diferencia entre poder resolver un gran problema y no poder resolverlo. Mucha de la tecnología digital y la tecnología de medios digitales que tenemos hoy es posible por ese rápido algoritmo. Otro ejemplo fue desarrollado por Andrew Appel, quien ahora es jefe de ciencias de la computación aquí en Princeton. Y fue desarrollado antes de graduarse mientras realizaba su tesis. Es un algoritmo rápido, para el problema de simulación de N cuerpos. El fácil algoritmo toma un tiempo proporcional a N^2, pero el de Appel era de N log N que nuevamente, significa que los científicos pueden hacer simulaciones de N cuerpos para enormes valores de N. Y eso permite nuevas investigaciones. Ahora, el reto es que usualmente enfrentarás es, ¿mi programa podrá manejar bien una gran cantidad de datos? Y, en realidad, el programador en su trabajo enfrenta ese reto todo el tiempo. ¿Porqué, porqué está corriendo tan lento mi programa? ¿Porqué se le acaba la memoria? Y eso es algo que los programadores han enfrentado por muchísimo tiempo, y la perspectiva para poder manejarlo, de Deuter Kanoof, en los 1970s, fue que podemos usar el método científico para entender el rendimiento de los algoritmos en práctica. Tal vez no estemos revelando nuevos secretos del universo, pero podemos usar el método científico, y tratar la computadora, como algo que se estudia de esa manera, y llegar a un entendimiento de cómo nuestro programa va a desempeñarse. Ahora, veamos eso en más detalle. Así que esto es sólo un breve resumen de lo que queremos decir con el método científico, el que ha sido exitoso por varios siglos ahora. Así que, lo que vamos a hacer es, observar desde el punto de vista de algo que exista en el mundo natural. En este caso, será el tiempo de corrida de nuestro programa en una computadora. Después desarrollaremos una hipótesis, algún modelo que sea consistente con nuestras observaciones, y se espera que esa hipótesis es lo suficientemente buena como para permitir que podamos predecir algo. Usualmente queremos predecir el tiempo de corrida para un problema más grande, o en un ordenador diferente. Y luego verificaremos las predicciones haciendo más observaciones, y validaremos hasta que estemos seguros que nuestro modelo, hipótesis y observaciones sean consistentes unos con los otros. Eso es una manera de asegurarse que entendemos el rendimiento de nuestros programas. Ahora, dentro del método científico, hay unos principios básicos y el primero es que, si vas a hacer experimentos, debes esperar que alguien más pueda hacer los mismos experimentos y obtener el mismo resultado. Además, la hipótesis debe tener la propiedad que el experimento pueda comprobarla o refutarla. Así que tiene que estar hecha cuidadosamente, y nos aseguraremos de tratar de hacer eso. Así que, otra vez, el futuro del mundo natural que estamos estudiamos es alguna computadora en particular que existe en el mundo natural. Cambia el algoritmo de una abstracción a alguna, alguna cosa física que ocurre, como electrones viajando rápidamente dentro de la computadora.