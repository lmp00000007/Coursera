1
00:00:02,263 --> 00:00:06,571
Okay, our basic array implementation
of stacks had the defect where

2
00:00:06,571 --> 00:00:12,050
we required clients to provide the maximum
capacity of the stack ahead of time.

3
00:00:12,050 --> 00:00:15,140
Now we're going to look at a technique for
resolving that problem.

4
00:00:16,850 --> 00:00:20,560
How do we, we do not implementing the API?

5
00:00:20,560 --> 00:00:23,330
The API says we should just
be able to create a stack and

6
00:00:23,330 --> 00:00:25,440
it should be able to grow and
shrink to any size.

7
00:00:25,440 --> 00:00:27,850
So how are we going to grow and
shrink the array?

8
00:00:28,860 --> 00:00:32,698
Well, first thing you might think of is,
when the client pushes a new item onto

9
00:00:32,698 --> 00:00:35,953
the stack, increase the size of
the array by 1, and when it pops,

10
00:00:35,953 --> 00:00:38,350
decrease the size of the array by 1.

11
00:00:38,350 --> 00:00:39,860
That's easy to code up, but

12
00:00:39,860 --> 00:00:43,250
not worth it, because it's
much too expensive to do that.

13
00:00:43,250 --> 00:00:47,300
The reason is that you have to create
a new array, size one bigger, and

14
00:00:47,300 --> 00:00:49,140
copy all the items to that new array.

15
00:00:50,240 --> 00:00:55,059
So, inserting the first N items
would take time proportional,

16
00:00:55,059 --> 00:01:00,875
if the stack's of size N-1,
it's going to take time N, N-2, time N-1.

17
00:01:00,875 --> 00:01:04,872
So first N items would take
the sum of the first N integers,

18
00:01:04,872 --> 00:01:07,660
which we know is about N squared over 2.

19
00:01:07,660 --> 00:01:11,482
Quadratic time to insert
N items into a stack,

20
00:01:11,482 --> 00:01:15,206
that kind of performance
is unacceptable for

21
00:01:15,206 --> 00:01:20,207
large problems, as we've seen,
as we will see many times.

22
00:01:20,207 --> 00:01:24,553
So the challenge is to do the resizing,
but

23
00:01:24,553 --> 00:01:29,401
somehow ensure that it
happens infrequently.

24
00:01:29,401 --> 00:01:33,016
So the well-known technique for
doing that,

25
00:01:33,016 --> 00:01:37,741
called repeated doubling,
is to, when the array fills up,

26
00:01:37,741 --> 00:01:43,440
create a new array of twice the size and
copy all the items over.

27
00:01:43,440 --> 00:01:47,160
Then we don't create new
arrays all that often.

28
00:01:47,160 --> 00:01:50,740
So here's an implementation of that.

29
00:01:50,740 --> 00:01:53,210
We start with an array of size 1.

30
00:01:53,210 --> 00:01:57,013
If we have a full stack,
which we know by testing N,

31
00:01:57,013 --> 00:02:02,231
which is the number of items in
the stack versus the array length, then

32
00:02:02,231 --> 00:02:08,169
we just resize the array into one of twice
the length before inserting the item.

33
00:02:08,169 --> 00:02:12,270
And how do we resize to a new capacity?

34
00:02:12,270 --> 00:02:18,050
We create a new array of that capacity and
just go ahead and copy our

35
00:02:18,050 --> 00:02:22,969
current stack into the first
half of that and then return it.

36
00:02:24,250 --> 00:02:28,016
And that will reset our instance variable,

37
00:02:28,016 --> 00:02:32,201
which is our stack,
to this new, bigger array.

38
00:02:32,201 --> 00:02:36,040
[COUGH] So, the idea and
the consequence of this is,

39
00:02:36,040 --> 00:02:42,202
if you insert N items into an array, into
a stack with this array representation,

40
00:02:42,202 --> 00:02:46,990
the time will be proportional to N,
not N squared.

41
00:02:46,990 --> 00:02:53,330
And the reason is that you only create
a new array every time it doubles.

42
00:02:53,330 --> 00:02:59,140
But by the time that it doubles, you've
inserted that many items into the stack.

43
00:02:59,140 --> 00:03:08,080
So on average, it's like adding
a cost of one to each operation.

44
00:03:08,080 --> 00:03:12,880
So if you just calculate the cost
of inserting the first N items,

45
00:03:12,880 --> 00:03:16,330
you're going to have instead of the sum
of the integers from to 1 to N,

46
00:03:16,330 --> 00:03:19,550
you're going to have the sum of
the powers of 2 from 1 to N.

47
00:03:19,550 --> 00:03:22,575
And that'll give a total cost of about 3N.

48
00:03:23,740 --> 00:03:28,110
So that's, in array accesses for
the copy, there's two array accesses.

49
00:03:28,110 --> 00:03:33,025
So to insert N items,
it's about three array accesses.

50
00:03:33,025 --> 00:03:35,841
This plot is another way of looking at it,

51
00:03:35,841 --> 00:03:41,760
which is the number of array accesses
taken as you implement push operations.

52
00:03:41,760 --> 00:03:46,540
Every time you hit a power of 2, you take
that many array accesses, but in a sense,

53
00:03:46,540 --> 00:03:50,210
you've already paid for
them by putting those items on the stack.

54
00:03:51,640 --> 00:03:54,040
So that's called amortized analysis,

55
00:03:54,040 --> 00:03:58,010
where we consider the total cost
averaged over all operations.

56
00:03:58,010 --> 00:04:02,366
And this is a fine example and
useful example,

57
00:04:02,366 --> 00:04:08,858
of amortized analysis to get
efficiency in a stack implementation.

58
00:04:08,858 --> 00:04:12,240
Now we have, what about the pop, we have
to think about how to shrink the array.

59
00:04:13,250 --> 00:04:16,593
So, you might think, well,
we doubled it when it was full,

60
00:04:16,593 --> 00:04:19,621
why don't we cut it in half
when it gets to be half full?

61
00:04:19,621 --> 00:04:21,310
We don't want the array to get too empty.

62
00:04:22,860 --> 00:04:30,647
Well, that one doesn't exactly work
because of a phenomenon called thrashing.

63
00:04:30,647 --> 00:04:34,855
If the client happens to do
push-pop-push-pop alternating when

64
00:04:34,855 --> 00:04:39,280
the array is full, then it's going to
be doubling, having, doubling,

65
00:04:39,280 --> 00:04:41,216
having, doubling, having.

66
00:04:41,216 --> 00:04:44,410
Creating new arrays on every operation.

67
00:04:44,410 --> 00:04:47,310
Take time proportional to N for
every operation, and

68
00:04:47,310 --> 00:04:49,040
therefore quadratic time for everything.

69
00:04:49,040 --> 00:04:50,489
So I don't want to do that.

70
00:04:51,490 --> 00:04:53,990
The efficient solution is to

71
00:04:53,990 --> 00:04:58,270
wait until the array gets
one-quarter full before you have it.

72
00:04:58,270 --> 00:05:00,890
And that's very easy to implement.

73
00:05:00,890 --> 00:05:03,550
We can just test if the array
is one quarter full, if it is,

74
00:05:03,550 --> 00:05:05,580
we resize it to half full.

75
00:05:05,580 --> 00:05:08,770
And so then at that point,
it's half full, and

76
00:05:08,770 --> 00:05:14,400
it can either grow by adding stuff or
shrink by subtracting stuff.

77
00:05:14,400 --> 00:05:17,790
But there won't be another
resizing array operation

78
00:05:17,790 --> 00:05:22,610
until it either gets totally full or
half again full.

79
00:05:23,850 --> 00:05:28,382
So the invariant of that is that
the array is always between 25% and

80
00:05:28,382 --> 00:05:30,346
100% full, number one.

81
00:05:30,346 --> 00:05:34,849
And number two, that every time you
resize, you've already paid for

82
00:05:34,849 --> 00:05:38,760
it in an amortized sense by inserting,
pushing or popping.

83
00:05:39,800 --> 00:05:44,758
So here's just what
happens to the array for

84
00:05:44,758 --> 00:05:47,840
our small client example.

85
00:05:47,840 --> 00:05:52,020
And you can see at the beginning,
it doubles from one to two to four, but

86
00:05:52,020 --> 00:05:56,619
once it gets to four, it stays, once it
gets to eight, it stays at that size for

87
00:05:56,619 --> 00:05:59,800
awhile even though
there's some operations.

88
00:05:59,800 --> 00:06:04,764
And it doesn't shrink back to four until
after there's only two items in there,

89
00:06:04,764 --> 00:06:06,959
and then it shrinks, and so forth.

90
00:06:06,959 --> 00:06:11,181
So array resizing doesn't
happen that often, but

91
00:06:11,181 --> 00:06:16,188
it's a very effective way of
implementing the stack API with

92
00:06:16,188 --> 00:06:23,182
an array where the client does not have to
provide the maximum capacity of the stack.

93
00:06:23,182 --> 00:06:29,490
But still, we're guaranteed that
the amount of the memory that we are use

94
00:06:29,490 --> 00:06:36,322
is always only a constant multiple of
the number of items actually on the stack.

95
00:06:36,322 --> 00:06:41,525
So, the analysis now says that
the average running time per

96
00:06:41,525 --> 00:06:46,417
operation for
whatever the sequence of operations is,

97
00:06:46,417 --> 00:06:52,159
the average running time is going to
be proportional to a constant.

98
00:06:53,880 --> 00:06:58,227
Now, there is a worst case, that is,
at the point when the stack doubles,

99
00:06:58,227 --> 00:07:00,170
it takes time proportional to N.

100
00:07:00,170 --> 00:07:05,990
So it's not quite as good
performance as we might like.

101
00:07:05,990 --> 00:07:11,420
But the advantage that we
get is very fast pushes and

102
00:07:11,420 --> 00:07:16,960
pops, just access array and increment it,
and very efficient for most operations.

103
00:07:16,960 --> 00:07:20,480
And for many, many clients,
that's an effective tradeoff to make.

104
00:07:22,470 --> 00:07:24,030
So what about memory usage?

105
00:07:24,030 --> 00:07:27,260
Well, this is the analysis
of memory usage for

106
00:07:27,260 --> 00:07:33,110
stacks, and it's actually
less memory than for strings.

107
00:07:33,110 --> 00:07:36,908
The amount used is between 8N and 32N,

108
00:07:36,908 --> 00:07:41,850
depending on how full the array is and

109
00:07:41,850 --> 00:07:47,260
just a quick analysis of the amount
of space that arrays take in Java.

110
00:07:47,260 --> 00:07:51,671
So, again, this analysis is just for
the stack itself, not for

111
00:07:51,671 --> 00:07:54,296
the strings, which the client owns.

112
00:07:56,246 --> 00:08:02,404
So, what are the tradeoffs between using
a resizing array versus a linked list?

113
00:08:02,404 --> 00:08:05,280
Those are two different
implementations of the same API,

114
00:08:05,280 --> 00:08:07,880
and the client can use
them interchangeably.

115
00:08:07,880 --> 00:08:09,630
Which one is better?

116
00:08:09,630 --> 00:08:14,460
In many situations, we're going to
have multiple implementations of APIs,

117
00:08:14,460 --> 00:08:17,690
and depending on properties
of the client program,

118
00:08:17,690 --> 00:08:21,588
we're going to have to choose which
one is the better one to use.

119
00:08:21,588 --> 00:08:27,077
So for linked lists, every operation
takes constant time in the worst case,

120
00:08:27,077 --> 00:08:29,230
that's a guarantee.

121
00:08:29,230 --> 00:08:33,580
But we have to use a little extra time and
space to deal with the links.

122
00:08:33,580 --> 00:08:35,420
So it's going to be slower.

123
00:08:36,470 --> 00:08:39,450
Resizing-array implementation.

124
00:08:39,450 --> 00:08:46,180
We have a good amortized time, so total
averaged over the whole process is good.

125
00:08:46,180 --> 00:08:51,590
We have less wasted space and probably
faster implementation of each operation.

126
00:08:52,640 --> 00:08:56,240
And so for some clients,
maybe that makes a difference.

127
00:08:56,240 --> 00:09:00,230
Perhaps you wouldn't want to use
a resizing-array implementation at

128
00:09:00,230 --> 00:09:02,780
the moment that your plane's coming in for
a landing.

129
00:09:02,780 --> 00:09:07,210
You wouldn't want it to all of a sudden
not implement some operation quickly.

130
00:09:07,210 --> 00:09:11,380
If you need that kind of order,
maybe in an internet switch where packets

131
00:09:11,380 --> 00:09:15,890
are coming through at a great rate, you
wouldn't want to be in a situation where

132
00:09:15,890 --> 00:09:19,930
you're missing some data because
something got slow all of a sudden.

133
00:09:19,930 --> 00:09:21,850
So that's a tradeoff
that the client can make.

134
00:09:21,850 --> 00:09:25,350
If I want that guarantee, if I want to be
sure that every operation's going to be

135
00:09:25,350 --> 00:09:28,910
fast, I'll use a linked list.

136
00:09:28,910 --> 00:09:33,460
And if I don't need that guarantee, if I
just care about the total amount of time,

137
00:09:33,460 --> 00:09:37,930
I'll probably use the resizing-array
because the total will be much less,

138
00:09:37,930 --> 00:09:40,830
because individual operations are fast.

139
00:09:40,830 --> 00:09:45,342
So even with these simple data structures,
we have really important

140
00:09:45,342 --> 00:09:50,421
tradeoffs that actually make a difference
in lots of practical situations.